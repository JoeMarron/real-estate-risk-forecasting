{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for VaR Predictions in the UK Residential Real Estate Market\n",
    "***\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import shapiro, chi2\n",
    "from arch import arch_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LSTM, Flatten, SimpleRNN\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import sys\n",
    "import vartests\n",
    "\n",
    "from hpi_data import HPI_data\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "matplotlib.style.use('seaborn-paper')\n",
    "plt.rcParams['savefig.facecolor']='white'\n",
    "plt.rcParams[\"figure.figsize\"] = (18,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Preprocessing & EDA\n",
    "### [House Price Index Data](https://www.gov.uk/government/collections/uk-house-price-index-reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi = HPI_data('UK-HPI-full-file-2023-03.csv',\n",
    "             'Ward_to_Local_Authority_District_to_County_to_Region_to_Country_Lookup_in_United_Kingdom.csv')\n",
    "\n",
    "average_price_df, detached_price_df, semidet_price_df, terraced_price_df, flat_price_df = hpi.get_data().get_property_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_list_str = ['Average Price', 'Average Detached Price', 'Average Semi-Detached Price',\n",
    "               'Average Terraced Price', 'Average Flat Price']\n",
    "n = 0\n",
    "for df in [average_price_df, detached_price_df, semidet_price_df, terraced_price_df, flat_price_df]:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for column in df:\n",
    "        ax.plot(df.index, df[column], marker='', color='#5A5A5A', linewidth=1, alpha=0.6)\n",
    "    ax.plot(df.index, df['England'], marker='', color='red', linewidth=3, alpha=0.8)\n",
    "    ax.plot(df.index, df['London'], marker='', color='#035962', linewidth=3, alpha=0.8)\n",
    "\n",
    "    ax.set_xlim(df.index.min()+dt.timedelta(days=-365),df.index.max()+dt.timedelta(days=1550))\n",
    "    num=0\n",
    "    for i in df.values[-1]:\n",
    "        name=list(df)[num]\n",
    "        num+=1\n",
    "        if name != 'England' and name != 'London':\n",
    "            if name == \"Yorkshire and The Humber\":\n",
    "                ax.text(df.index.max()+dt.timedelta(days=30), i-5000, name,\n",
    "                        horizontalalignment='left', size='small', color='#5A5A5A')\n",
    "            else:\n",
    "                ax.text(df.index.max()+dt.timedelta(days=30), i, name,\n",
    "                        horizontalalignment='left', size='small', color='#5A5A5A')\n",
    "\n",
    "    cur_england = df['England'].tail(1)\n",
    "    cur_london = df['London'].tail(1)\n",
    "\n",
    "    # And add a special annotation for the group we are interested in\n",
    "    ax.text(df.index.max()+dt.timedelta(days=60), cur_england,\n",
    "            'England (%s)' % (f'£{int(cur_england/1000):,}k'), horizontalalignment='left', color='red')\n",
    "    ax.text(df.index.max()+dt.timedelta(days=60), cur_london,\n",
    "            'London (%s)' % (f'£{int(cur_london/1000):,}k'), horizontalalignment='left', color='#035962')\n",
    "\n",
    "    y_labels = [f'£{int(amt/1000):,}k' for amt in ax.get_yticks()]\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    sns.despine()\n",
    "\n",
    "    plt.xlabel('Date', fontweight='bold')\n",
    "    plt.ylabel(df_list_str[n], fontweight='bold')\n",
    "    plt.savefig(\"plots/\" + df_list_str[n].lower().replace(\" \", \"_\") + \"_lineplot.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns Dist Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "average_price_regions_df = average_price_df.loc[:, average_price_df.columns != \"England\"] \n",
    "average_price_country_df = average_price_df[\"England\"]\n",
    "\n",
    "for i, column in enumerate(average_price_regions_df.columns, 1):\n",
    "    dist = np.log1p(average_price_regions_df[column].pct_change().dropna())\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.xlim(-0.1,0.1)\n",
    "    plt.xticks([-0.1, -0.05, 0.0, 0.05, 0.1], [-0.1, -0.05, 0.0, 0.05, 0.1])\n",
    "    plt.xlabel(\" \")\n",
    "    if i == 8:\n",
    "        plt.xlabel(\"Log Returns\", fontsize=15)\n",
    "    plt.ylim(0,70)\n",
    "    plt.yticks([20, 40, 60], [20, 40, 60])\n",
    "    plt.ylabel(\" \")\n",
    "    if i == 4:\n",
    "        plt.ylabel(\"Frequency\", fontsize=15)\n",
    "    plt.text(0.049, 65, 'p = {:0.2e}'.format(shapiro(dist.reset_index()[column])[1]), fontweight=\"bold\")\n",
    "    plt.title(column, fontweight=\"bold\")\n",
    "    sns.histplot(dist, color='#035962', edgecolor=\"w\", alpha=1)\n",
    "sns.despine()\n",
    "plt.savefig(\"plots/\" + 'average_price_regional_distributions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.log1p(average_price_country_df.pct_change().dropna())\n",
    "plt.xlim(-0.07,0.07)\n",
    "plt.xticks([-0.05, -0.025, 0.0, 0.025, 0.05], [-0.05, -0.025, 0.0, 0.025, 0.05])\n",
    "plt.xlabel(\"Log Returns\", fontsize=15)\n",
    "\n",
    "plt.ylim(0,70)\n",
    "plt.yticks([20, 40, 60], [20, 40, 60])\n",
    "plt.ylabel(\"Frequency\", fontsize=15)\n",
    "plt.text(0.059, 67, 'p = {:0.2e}'.format(shapiro(dist.reset_index()['England'])[1]), fontweight=\"bold\")\n",
    "plt.title('England', fontweight=\"bold\")\n",
    "sns.histplot(dist, color='#035962', edgecolor=\"w\", alpha=1)\n",
    "sns.despine()\n",
    "plt.savefig(\"plots/\" + 'average_price_national_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [average_price_df, detached_price_df, semidet_price_df, terraced_price_df, flat_price_df]:\n",
    "    print(dict(average_price_df.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Returns DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_returns_df, detached_returns_df, semidet_returns_df, terraced_price_df, flat_returns_df = HPI_data.get_hpi_returns([average_price_df, detached_price_df, semidet_price_df, terraced_price_df, flat_price_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Average Price Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_returns_df_plot = average_returns_df.loc[:, average_returns_df.columns != \"England\"]\n",
    "for i, column in enumerate(average_returns_df_plot.columns, 1):\n",
    "    plt.subplot(3,3,i)\n",
    "    average_returns_df[column].plot(color='#035962')\n",
    "    plt.xlabel(\" \")\n",
    "    plt.ylim(-0.085,0.085)\n",
    "    if i == 8:\n",
    "        plt.xlabel(\"Date\", fontsize=15)\n",
    "    #plt.ylim(0,70)\n",
    "    #plt.yticks([20, 40, 60], [20, 40, 60])\n",
    "    plt.ylabel(\" \")\n",
    "    if i == 4:\n",
    "        plt.ylabel(\"Returns\", fontsize=15)\n",
    "    plt.title(column, fontweight=\"bold\")\n",
    "\n",
    "sns.despine()\n",
    "plt.savefig(\"plots/\" + 'average_price_returns.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Value-at-Risk\n",
    "\n",
    "### Filtered Historic Simulation (GJR-GARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def garch_model(returns, vol=\"Garch\", p=1, o=1, q=1, dist=\"normal\"):\n",
    "    return arch_model(returns, vol=vol, p=p, o=o, q=q, dist=dist)\n",
    "\n",
    "\n",
    "def FHS_VaR(returns, col, garch_model):\n",
    "    \n",
    "    first_obs = returns.index[0]\n",
    "    last_obs = returns.index[-2]\n",
    "    start_fc = returns.index[-1]\n",
    "    \n",
    "    res = garch_model.fit(disp='off', last_obs=last_obs)\n",
    "    \n",
    "    forecasts = res.forecast(start=start_fc, reindex=False, horizon=1)\n",
    "    cond_mean = forecasts.mean[start_fc:]\n",
    "    cond_var = forecasts.variance[start_fc:]\n",
    "    \n",
    "    std_rets = ((returns[:last_obs][col] - res.params[\"mu\"])) / res.conditional_volatility\n",
    "    std_rets = std_rets.dropna()\n",
    "    q = std_rets.quantile([0.05])\n",
    "    \n",
    "    value_at_risk = -(cond_mean.values - np.sqrt(cond_var).values * q.values[None, :])\n",
    "    value_at_risk = pd.DataFrame(value_at_risk, columns=[\"VaR 95%\"], index=cond_var.index)    \n",
    "    \n",
    "    value_at_risk['actual_return'] = returns[-1:]\n",
    "    \n",
    "    return value_at_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate VaR Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ignore_warnings(category=ConvergenceWarning)\n",
    "def generate_VaR_observations(prices, column, prop, model_window, no_observations):\n",
    "    \n",
    "    df = pd.DataFrame(prices[column])\n",
    "    returns_ls = []\n",
    "    VaRs = []\n",
    "    \n",
    "    if not no_observations:\n",
    "        no_obs = len(df)-model_window-1\n",
    "    else:\n",
    "        no_obs = no_observations\n",
    "    \n",
    "    for i in tqdm(range(no_obs), desc=\"Loading…\",ascii=False, ncols=75, position=0, leave=True):\n",
    "        if no_observations == False:\n",
    "            period = df.iloc[i:i+model_window, :]\n",
    "        else:\n",
    "            nrows = range(df.shape[0])\n",
    "            ix = random.randint(nrows.start, nrows.stop-model_window-1)\n",
    "            period = df.iloc[ix:ix+model_window, :]       \n",
    "        scaler = StandardScaler()\n",
    "        returns = pd.DataFrame(scaler.fit_transform(period), columns=period.columns, index=period.index)\n",
    "        gm = garch_model(returns)\n",
    "        VaRs.append(FHS_VaR(returns[int((model_window-1)/2+1):], column, gm))\n",
    "        returns_ls.append(returns[int((model_window-1)/2+1):].values)\n",
    "    VaRs = pd.concat(VaRs)\n",
    "    \n",
    "    x = []\n",
    "    for r in returns_ls:\n",
    "        x.append(np.array(r.flatten()))\n",
    "    VaRs['x'] = x\n",
    "    VaRs = VaRs.sort_index()\n",
    "    VaRs = VaRs.reset_index()\n",
    "    \n",
    "    VaRs['region'] = column\n",
    "    VaRs['property']   = prop\n",
    "\n",
    "    return VaRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate VaR DataFrame Lists (Filter Regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_VaRs, detached_VaRs, semidet_VaRs, terraced_VaRs, flat_VaRs = [], [], [], [], []\n",
    "\n",
    "regions = ['England', 'London', 'South West', 'West Midlands', 'Yorkshire and The Humber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions:\n",
    "\n",
    "    print('Generating VaR Observations for %s:' % region)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    average_VaRs.append(generate_VaR_observations(average_returns_df, region, 'average', 49, False))\n",
    "    detached_VaRs.append(generate_VaR_observations(detached_returns_df, region, 'detached', 49, False))\n",
    "    semidet_VaRs.append(generate_VaR_observations(semidet_returns_df, region, 'semi_detached', 49, False))\n",
    "    terraced_VaRs.append(generate_VaR_observations(terraced_returns_df, region, 'terraced', 49, False))\n",
    "    flat_VaRs.append(generate_VaR_observations(flat_returns_df, region, 'flat', 49, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prop in [average_VaRs, detached_VaRs, semidet_VaRs, terraced_VaRs, flat_VaRs]:\n",
    "    for region in prop:\n",
    "        region = region.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine VaR DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([average_VaRs[0], detached_VaRs[0], semidet_VaRs[0], terraced_VaRs[0], flat_VaRs[0],\n",
    "                average_VaRs[1], detached_VaRs[1], semidet_VaRs[1], terraced_VaRs[1], flat_VaRs[1],\n",
    "                average_VaRs[2], detached_VaRs[2], semidet_VaRs[2], terraced_VaRs[2], flat_VaRs[2],\n",
    "                average_VaRs[3], detached_VaRs[3], semidet_VaRs[3], terraced_VaRs[3], flat_VaRs[3],\n",
    "                average_VaRs[4], detached_VaRs[4], semidet_VaRs[4], terraced_VaRs[4], flat_VaRs[4]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VaR Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_VaR(df, \n",
    "             VaR_col=[\"VaR 95%\", \"Predictions\"],\n",
    "             return_col = \"actual_return\",\n",
    "             VaR_colour = [\"#035962\", \"#8B0000\"],\n",
    "             markers = {\"#000000\": \"o\", \"red\": \"x\", \"#00C500\": \"x\", \"#EC91D8\": \"x\"},\n",
    "             labels = {\"#000000\": \"No Breach\", \"red\": \"Predicted breach\", \"#00C500\": \"Correct Breach\", \"#EC91D8\": \"Prevented breach\"},\n",
    "             sizes = {\"#000000\": 18, \"red\": 80, \"#00C500\": 80, \"#EC91D8\": 80},\n",
    "             single = False,\n",
    "             save_name=None,\n",
    "             print_plot=False):\n",
    "    \n",
    "    \n",
    "    if len(VaR_col) == 2:\n",
    "        df = df[[return_col, VaR_col[0], VaR_col[1]]]\n",
    "        df = df.groupby(df.index).mean()\n",
    "        #df = df.reset_index()\n",
    "        \n",
    "        ax = df.plot(y=VaR_col, legend=False, linestyle = \"--\",\n",
    "                     linewidth = 1.5, style=VaR_colour)\n",
    "        \n",
    "\n",
    "        df = df.sort_index()\n",
    "        xlim = ax.set_xlim(df.index[0], df.index[-1])\n",
    "        ylim = ax.set_ylim(df.min(numeric_only=True).min()*1.025,\n",
    "                           df.max(numeric_only=True).max()*1.025)\n",
    "\n",
    "        point_colours = []\n",
    "        for i, x in df.iterrows():\n",
    "            if x[return_col] > x[VaR_col[0]]:\n",
    "                if x[return_col] > x[VaR_col[1]]:\n",
    "                    point_colours.append(list(markers.keys())[0])\n",
    "                else:\n",
    "                    point_colours.append(list(markers.keys())[1])\n",
    "            else:\n",
    "                if x[return_col] <= x[VaR_col[1]]:\n",
    "                    point_colours.append(list(markers.keys())[2])\n",
    "                else:\n",
    "                    point_colours.append(list(markers.keys())[3])\n",
    "\n",
    "        point_colours = np.array(point_colours, dtype=\"object\")\n",
    "        for colour in np.unique(point_colours):\n",
    "            \n",
    "            sel = point_colours == colour\n",
    "            if sel.sum() > 1:\n",
    "\n",
    "                ax.scatter(\n",
    "                    x=df.index[sel],\n",
    "                    y=df[return_col][sel],\n",
    "                    marker=markers[colour],\n",
    "                    c=point_colours[sel],\n",
    "                    label=labels[colour],\n",
    "                    s=sizes[colour])\n",
    "            plt.axhline(y=0, color='r', linestyle='--', alpha=0.4)\n",
    "            ax.set_xlabel('Date', fontweight='bold')\n",
    "            ax.set_ylabel('Returns', fontweight='bold')\n",
    "            leg = ax.legend(frameon=True, ncol=3, loc='upper left', fontsize=11)\n",
    "\n",
    "    \n",
    "        \n",
    "        if save_name != None:\n",
    "            plt.savefig('%s.png' % save_name)\n",
    "        \n",
    "        if print_plot==True:\n",
    "            plt.show()\n",
    "            \n",
    "        plt.close()\n",
    "    \n",
    "    else:\n",
    "\n",
    "        df = df[[\"Date\", return_col, VaR_col]]\n",
    "\n",
    "        df = df.groupby(['Date']).mean()\n",
    "        df = df.reset_index()\n",
    "\n",
    "\n",
    "        ax = df.plot(x=\"Date\",\n",
    "                      y=VaR_col,\n",
    "                      legend=False,\n",
    "                      linestyle = \"--\",\n",
    "                      linewidth = 1.5,\n",
    "                      style=VaR_colour)\n",
    "\n",
    "        df = df.sort_values('Date')\n",
    "        xlim = ax.set_xlim(df['Date'].min(), df['Date'].max())\n",
    "        ylim = ax.set_ylim(df.min(numeric_only=True).min()*1.025,\n",
    "                           0)\n",
    "                           #df.max(numeric_only=True).max()*1.025)\n",
    "\n",
    "        point_colours = []\n",
    "        for i, x in df.iterrows():\n",
    "            if x[return_col] > x[VaR_col]:\n",
    "                point_colours.append(list(markers.keys())[0])\n",
    "            else:\n",
    "                point_colours.append(list(markers.keys())[1])\n",
    "\n",
    "        point_colours = np.array(point_colours, dtype=\"object\")\n",
    "\n",
    "        for colour in np.unique(point_colours):\n",
    "\n",
    "            sel = point_colours == colour\n",
    "            if sel.sum() > 1:\n",
    "\n",
    "                ax.scatter(\n",
    "                    df['Date'].iloc[sel],#.index[sel],\n",
    "                    df[return_col].iloc[sel],\n",
    "                    marker=markers[colour],\n",
    "                    c=point_colours[sel],\n",
    "                    label=labels[colour],\n",
    "                    s=sizes[colour])\n",
    "            plt.axhline(y=0, color='r', linestyle='--', alpha=0.4)\n",
    "            ax.set_xlabel('Date', fontweight='bold')\n",
    "            ax.set_ylabel('Returns', fontweight='bold')\n",
    "            leg = ax.legend(frameon=True, ncol=3, loc='upper left')\n",
    "  \n",
    "\n",
    "        if save_name != None:\n",
    "            plt.savefig(\"plots/\" + '%s.png' % save_name)\n",
    "\n",
    "        if print_plot==True:\n",
    "            plt.show()\n",
    "            \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Neural Networks\n",
    "\n",
    "### ANN, RNN and RNN-LSTM Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann(hidden_units=64, learning_rate=0.001, num_hidden_layers=2, drop_out=0.1):\n",
    "    \n",
    "    model = keras.Sequential() # initiate model\n",
    "    \n",
    "    model.add(Flatten(input_shape=(24,))) # first layer\n",
    "    \n",
    "    # add hidden layers\n",
    "    for _ in range(num_hidden_layers-1):\n",
    "        model.add(Dense(hidden_units, activation='relu'))\n",
    "        model.add(Dropout(drop_out))\n",
    "\n",
    "    model.add(Dense(1)) # output layer\n",
    "    \n",
    "    # compile (optimiser=Adam, loss=MSE, metric=MAE) with given learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "def build_rnn(hidden_units=64, learning_rate=0.001, num_hidden_layers=2, drop_out=0.1):\n",
    "    \n",
    "    model = keras.Sequential()  # initiate model\n",
    "    \n",
    "    # first RNN layer\n",
    "    model.add(SimpleRNN(units=hidden_units, activation = \"relu\", \n",
    "                                     return_sequences = True, input_shape = (24,1, )))\n",
    "    model.add(Dropout(drop_out)) # add dropout\n",
    "    \n",
    "    # add hidden layers\n",
    "    if num_hidden_layers > 2:\n",
    "        for _ in range(num_hidden_layers-2):\n",
    "            model.add(SimpleRNN(units=hidden_units, activation='relu', return_sequences=True))\n",
    "            model.add(Dropout(drop_out))        \n",
    "    model.add(SimpleRNN(units=hidden_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1)) # output layer\n",
    "    \n",
    "    # compile (optimiser=Adam, loss=MSE, metric=MAE) with given learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mean_absolute_error']) \n",
    "    return model\n",
    "\n",
    "def build_lstm(hidden_units=64, learning_rate=0.001, num_hidden_layers=2, drop_out=0.1):\n",
    "    \n",
    "    model = keras.Sequential() # initiate model\n",
    "    \n",
    "    # Add LSTM layer\n",
    "    model.add(LSTM(units=hidden_units, return_sequences=True, input_shape=(24,1,)))\n",
    "    \n",
    "    model.add(Dropout(drop_out)) # add drop out\n",
    "    \n",
    "    # add hidden layers\n",
    "    if num_hidden_layers > 2:\n",
    "        for _ in range(num_hidden_layers-2):\n",
    "            model.add(SimpleRNN(units=hidden_units, activation='relu', return_sequences=True))\n",
    "            model.add(Dropout(drop_out))\n",
    "    model.add(SimpleRNN(units=hidden_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1)) # output layer\n",
    "    \n",
    "    # compile (optimiser=Adam, loss=MSE, metric=MAE) with given learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mean_absolute_error']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Train Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df, date_col='Date', region=\"England\", prop=\"average\", x_col='x', y_col='1%',\n",
    "               random=False, test_size=0.2, seed=42):\n",
    "    \n",
    "    if random:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df[x_col], df[y_col],\n",
    "                                                            test_size=test_size, random_state=seed)\n",
    "    else:\n",
    "        train = df[(df['region']!=region) & (df['property']!=prop)]\n",
    "        test  = df[(df['region']==region) & (df['property']==prop)]\n",
    "\n",
    "        #df.query('20130101 < date < 20130201')\n",
    "        x_train = train[x_col]\n",
    "        x_test  = test[x_col]\n",
    "        y_train = train[y_col]\n",
    "        y_test  = test[y_col]\n",
    "        \n",
    "    #print(x_train.shape, x_test.shape, y_train.shape, y_test.shape )\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NN Modelling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimisation(x_train, y_train, param_grid, nn_func=build_ann, epochs=100, bs=64):\n",
    "    \n",
    "    # initiate early stopping\n",
    "    early_stopping = EarlyStopping(monitor='mean_absolute_error', patience=6)\n",
    "    \n",
    "    # use tensorflow scikit learn wrapper for Keras Regressor\n",
    "    # pass NN model, epchs, batch size and early stopping callback\n",
    "    model = KerasRegressor(build_fn=nn_func, epochs=epochs, batch_size=bs, verbose=0, callbacks=[early_stopping])\n",
    "    \n",
    "    # Initiate pipeline: pass model wrapper and scaler function\n",
    "    pipe = Pipeline([('scale', StandardScaler()),('model', model)])\n",
    "    \n",
    "    # pass pipe to TransformedTargetRegressor wrapper and standard scaler transformer\n",
    "    # this is to utilise scaling of y values\n",
    "    estimator = TransformedTargetRegressor(regressor=pipe, transformer=StandardScaler())\n",
    "    \n",
    "    # Initiate RandomizedSearchCV (3-Fold, 5-Iterations)\n",
    "    grid = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, cv=3,\n",
    "                        verbose=1, scoring='neg_mean_squared_error', n_iter = 5)\n",
    "    \n",
    "    # Fit and run RandomizedSearchCV\n",
    "    grid_result = grid.fit(x_train, y_train)\n",
    "    \n",
    "    #return result of model training\n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optimal_model_loss(grid_result, print_plot=False, plot_name=None):\n",
    "    \n",
    "    hist = grid_result.best_estimator_.regressor_.named_steps['model'].model.history\n",
    "    \n",
    "    mse  = hist.history['loss']\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = hist.history['mean_absolute_error']\n",
    "\n",
    "    epoch_count = range(1, len(mse) + 1)\n",
    "\n",
    "    # Visualize loss history\n",
    "    plt.plot(epoch_count, rmse, 'b-.')\n",
    "    plt.plot(epoch_count, mse, 'r--')\n",
    "    plt.plot(epoch_count, mae, '#035962')\n",
    "    \n",
    "    plt.legend(['RMSE', 'MSE', 'MAE'], fontsize=13)\n",
    "    plt.xlabel('Epoch', fontweight='bold', fontsize=13)\n",
    "    plt.ylabel('Loss', fontweight='bold', fontsize=13)\n",
    "    \n",
    "    plt.text(epoch_count[-1]+0.2, rmse[-1], round(rmse[-1], 2), bbox=dict(facecolor='white', edgecolor='b'))\n",
    "    plt.text(epoch_count[-1]+0.2, mse[-1], round(mse[-1], 2), bbox=dict(facecolor='white', edgecolor='r'))\n",
    "    plt.text(epoch_count[-1]+0.2, mae[-1], round(mae[-1], 2), bbox=dict(facecolor='white', edgecolor='#035962'))\n",
    "    \n",
    "    sns.despine()\n",
    "    \n",
    "    if plot_name != None:\n",
    "        plt.savefig(\"%s.png\" % plot_name)\n",
    "    \n",
    "    if print_plot:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_evaluation(model, test_df, test_x, actual_col, return_col, print_plot=False, plot_name=None):\n",
    "    \n",
    "    # retrieve optimal model from RandomizedSearchCV\n",
    "    optim_model = model.best_estimator_\n",
    "\n",
    "    # make predictions\n",
    "    predictions = optim_model.predict(np.array(test_x.tolist()).astype('float64'))\n",
    "    \n",
    "    eval_df = pd.DataFrame(test_df) #get test dataframe\n",
    "    eval_df['Predictions'] = predictions # assign predictions to column\n",
    "    eval_df = eval_df.groupby(eval_df['Date']).mean() # group by date\n",
    "    \n",
    "    # calculate breach percentage of VaR and predicted VaR\n",
    "    pred_backtest = round(((eval_df['Predictions'] > eval_df[return_col]).sum() / len(eval_df))*100, 2)\n",
    "    actual_backtest = round(((eval_df[actual_col] > eval_df[return_col]).sum() / len(eval_df))*100, 2)\n",
    "    \n",
    "    # calculate RMSE\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_pred=eval_df['Predictions'], y_true=eval_df[actual_col])),3)\n",
    "    \n",
    "    # Plot VaR plot using function\n",
    "    plot_VaR(eval_df, save_name=plot_name, print_plot=print_plot)\n",
    "    \n",
    "    #Return Violations as binary output for Act and Pred VaR\n",
    "    eval_df['actual_violation'] = eval_df.apply(lambda x: 1 if x[actual_col] >= x[return_col] else 0, axis=1)\n",
    "    eval_df['pred_violation'] = eval_df.apply(lambda x: 1 if x['Predictions'] >= x[return_col] else 0, axis=1)\n",
    "    \n",
    "    # Use this to calculate Kupiec test (1 dof) for both\n",
    "    actl_kup = vartests.kupiec_test(eval_df['actual_violation'], var_conf_level=0.95, conf_level=0.95)['log-likelihood']\n",
    "    pred_kup = vartests.kupiec_test(eval_df['pred_violation'], var_conf_level=0.95, conf_level=0.95)['log-likelihood']\n",
    "    \n",
    "    # get p-value from kupiec test\n",
    "    actl_kup, pred_kup = round(chi2.pdf(actl_kup, 1), 4), round(chi2.pdf(pred_kup, 1), 4)\n",
    "    \n",
    "    # return results for final results summary dataframe\n",
    "    return [rmse, actual_backtest, pred_backtest, actl_kup, pred_kup, model.best_params_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Running NN Modelling\n",
    "\n",
    "### Hyperparameter Grid and Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'regressor__model__hidden_units'      :    [32, 64, 128],\n",
    "              'regressor__model__learning_rate'     :     [1e-2, 1e-3],\n",
    "              'regressor__model__num_hidden_layers' :           [3, 4],\n",
    "              'regressor__model__drop_out'          : [0.05, 0.1, 0.2]} \n",
    "\n",
    "\n",
    "properties = ['average', 'detached', 'semi_detached', 'terraced', 'flat']\n",
    "\n",
    "model_names = ['ANN', 'RNN', 'LSTM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Modelling for each Region & Property Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for region in regions:\n",
    "    for prop in properties:\n",
    "        n = 0\n",
    "        for model_func in [build_ann, build_rnn, build_lstm]:\n",
    "            file_region = \"%s_%s\" % (prop, region.replace(\" \", \"_\"))\n",
    "            file_model  = model_names[n]\n",
    "            \n",
    "            print('~~~~~~~~~~~~~~~~~~~~ %s - %s - %s ~~~~~~~~~~~~~~~~~~~~' % (model_names[n], prop, region))\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test(full_df, region=region, prop=prop, y_col='VaR 95%')\n",
    "            \n",
    "            # run optimisation\n",
    "            cv_results = run_optimisation(np.array(x_train.tolist()).astype('float64'), np.array(\n",
    "                y_train.values).astype('float64'), param_grid, nn_func=model_func, epochs=200, bs=128)\n",
    "\n",
    "            # plot best model loss plot from cv results above - save plot to folder\n",
    "            plot_optimal_model_loss(cv_results, print_plot=False,\n",
    "                plot_name= '%s_%s_optimal_model_loss_plt' % (file_region, file_model))\n",
    "\n",
    "            # make predictions on test data, return performance metrics - save backtest plot to folder\n",
    "            results = best_model_evaluation(cv_results, full_df.loc[x_test.index], x_test,\n",
    "                'VaR 95%', 'actual_return', print_plot=True, \n",
    "                plot_name= '%s_%s_var_prediction_backtest_plot' % (file_region, file_model))\n",
    "\n",
    "            # append model stats to list for later evaluation\n",
    "            final_results.append([prop, region, model_names[n],\n",
    "                                    results[0], results[1], results[2], results[3],\n",
    "                                    results[4], results[5]])\n",
    "\n",
    "            n += 1\n",
    "\n",
    "# write output of model predictions evaluations to dataframe\n",
    "final_results = pd.DataFrame(final_results, columns=['property', 'region', 'model',\n",
    "                                                          'rmse', 'btest_actual', 'btest_predict',\n",
    "                                                          'act_pval', 'pred_pval',\n",
    "                                                          'best_params'])\n",
    "\n",
    "# explode dict of best params into seperate columns\n",
    "final_results = pd.concat([final_results.drop(['best_params'], axis=1),\n",
    "                              final_results['best_params'].apply(pd.Series)], axis=1)\n",
    "final_results.columns = [x.replace('regressor__model__', '') for x in final_results.columns]\n",
    "\n",
    "# save final results to folder\n",
    "final_results.to_csv('final_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_results.to_csv('final_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.read_csv('final_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average RMSE for each NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.groupby(['model'])['rmse'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average RMSE for each Region and Property Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = final_results.groupby(['property', 'region'])['rmse'].mean().reset_index()\n",
    "rmses = round(pd.pivot_table(rmses, values = 'rmse', index='region', columns = 'property').reset_index(), 4)\n",
    "rmses['Mean'] = round(rmses.mean(axis=1), 4)\n",
    "rmses.loc[5] = rmses.mean(axis=0)\n",
    "rmses.loc[5]['region'] = 'Mean'\n",
    "rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kupiec Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(final_results.groupby(['region'])[['act_pval', 'pred_pval']].mean().reset_index(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.groupby(['property'])[['act_pval', 'pred_pval']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(df):\n",
    "    summ = df.describe()[regions]\n",
    "    skew = pd.DataFrame(df.skew()).T[regions]\n",
    "    skew = skew.rename(index={0:'skewness'})\n",
    "    summ.loc[\"skewness\"] = skew.loc['skewness']\n",
    "    kurt = pd.DataFrame(df.kurt()).T[regions]\n",
    "    kurt = kurt.rename(index={0:'kurtosis'})\n",
    "    summ.loc[\"kurtosis\"] = kurt.loc['kurtosis']\n",
    "    summ = summ.T\n",
    "    summ = summ[['count', 'min', 'mean', 'max', 'std', 'skewness', 'kurtosis']]\n",
    "    summ['count'] = pd.to_numeric(summ['count'], downcast='integer')\n",
    "    \n",
    "    return round(summ, 3).reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
